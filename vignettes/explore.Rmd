---
title: "Exploring timing data"
author: "Kirill MÃ¼ller"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Exploring timing data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r}
library(wrswoR)
library(ggplot2)
library(dplyr)
library(tidyr)
```

# Run time tests

- Input sizes: `r unique(timings$n)`
- Output ratios: `r scales::percent(unique(timings$r))`
- Probability distributions: `r unique(timings$prob)`
- Algorithms: `r unique(timings$expr)`
- Total measurements: `r nrow(timings)`

Clearly worse performance of R implementation for large inputs.
(Default output ratio 10% for now.)

```{r fig.width=7, fig.height=5}
timings %>%
  filter(r == 0.1) %>%
  filter(prob == "uniform") %>%
  ggplot(aes(x=factor(n), y=time * 1e-9, color=expr)) +
  geom_boxplot() +
  scale_y_log10()
```

Compare median and worst performance (for uniform distribution).

```{r fig.width=7, fig.height=5}
timings %>%
  filter(r == 0.1) %>%
  filter(prob == "uniform") %>%
  group_by(n, expr) %>%
  summarize(median = median(time), max = max(time)) %>%
  ungroup %>%
  gather(metric, time, median, max) %>%
  ggplot(aes(x=n, y=time * 1e-9, color=expr)) +
  facet_wrap(~metric, ncol = 1) +
  geom_line() +
  scale_x_log10() +
  scale_y_log10()
```

Worst-best comparison: best for R, worst for others (but per probability distribution).

```{r}
best_worst <- function(expr, time) {
  if (expr[[1]] == "R") min(time) else max(time)
}
```

```{r fig.width=7, fig.height=5}
timings %>%
  filter(r == 0.1) %>%
  filter(n >= 100 & n < 10000) %>%
  group_by(n, expr, prob) %>%
  summarize(minmax = best_worst(expr, time)) %>%
  ungroup %>%
  gather(metric, time, minmax) %>%
  ggplot(aes(x=n, y=time * 1e-9, color=expr)) +
  facet_wrap(~prob, ncol = 1) +
  geom_line() +
  scale_x_log10() +
  scale_y_log10()
```

Same as above, now taking 5% and 95% quantiles.

```{r}
best_worst_q <- function(expr, time) {
  if (expr[[1]] == "R") quantile(time, 0.05) else quantile(time, 0.95)
}
```

```{r fig.width=7, fig.height=5}
timings %>%
  filter(r == 0.1) %>%
  filter(n >= 100 & n <= 10000) %>%
  group_by(n, expr, prob) %>%
  summarize(minmax = best_worst_q(expr, time)) %>%
  ungroup %>%
  gather(metric, time, minmax) %>%
  ggplot(aes(x=n, y=time * 1e-9, color=expr)) +
  facet_wrap(~prob, ncol = 1) +
  geom_line() +
  scale_x_log10() +
  scale_y_log10()
```

## More detailed data for break-even analysis

Min-max very sensitive to garbage collection
(see also https://radfordneal.wordpress.com/2014/02/02/inaccurate-results-from-microbenchmark/):

```{r fig.width=7, fig.height=5}
break_even %>%
  filter(r == 0.1) %>%
  group_by(n, expr, prob) %>%
  summarize(minmax = best_worst(expr, time)) %>%
  ungroup %>%
  gather(metric, time, minmax) %>%
  ggplot(aes(x=n, y=time * 1e-9, color=expr)) +
  facet_wrap(~prob, ncol = 1) +
  geom_line() +
  scale_x_log10() +
  scale_y_log10()
```

Better take a quantile (say, 95th):

```{r fig.width=7, fig.height=5}
break_even %>%
  filter(r == 0.1) %>%
  filter(n < 3000) %>%
  group_by(n, expr, prob) %>%
  summarize(minmax = best_worst_q(expr, time)) %>%
  ungroup %>%
  gather(metric, time, minmax) %>%
  ggplot(aes(x=n, y=time * 1e-9, color=expr)) +
  facet_wrap(~prob, ncol = 1) +
  geom_line() +
  scale_x_log10() +
  scale_y_log10()
```

What about the mean:

```{r fig.width=7, fig.height=5}
break_even %>%
  filter(r == 0.1) %>%
  filter(n >= 100) %>%
  group_by(n, expr, prob) %>%
  summarize(mean = mean(time)) %>%
  ungroup %>%
  gather(metric, time, mean) %>%
  ggplot(aes(x=n, y=time * 1e-9, color=expr)) +
  facet_wrap(~prob, ncol = 1) +
  geom_line() +
  scale_x_log10() +
  scale_y_log10()
```

R is best with uniform distribution (compared to the others);
compare different ratios:

```{r fig.width=7, fig.height=5}
break_even %>%
  filter(prob == "uniform") %>%
  filter(n >= 100) %>%
  group_by(n, expr, r) %>%
  summarize(mean = mean(time)) %>%
  ungroup %>%
  gather(metric, time, mean) %>%
  mutate(percent_r = scales::percent(r)) %>%
  ggplot(aes(x=n, y=time * 1e-9, color=expr)) +
  facet_wrap(~percent_r, ncol = 1) +
  geom_line() +
  scale_x_log10() +
  scale_y_log10()
```


Even better, avoid allocation of R vectors in Rcpp code, as done by `ccrank`.


## Compare distributions

Box plots for relative differences beween non-uniform and uniform sampling.

```{r fig.width=7, fig.height=5}
timings %>%
  filter(r == 0.1) %>%
  arrange(prob, n, expr, time) %>%
  group_by(prob, n, expr) %>%
  mutate(id = seq_along(time)) %>%
  ungroup %>%
  spread(prob, time) %>%
  gather_("diff_to", "other_time", unique(timings$prob) %>% setdiff("uniform")) %>%
  mutate(time_diff = (other_time - uniform) / uniform, uniform = NULL, other_time = NULL) %>%
  ggplot(aes(x=factor(n), y=time_diff, color=expr)) +
  facet_wrap(~diff_to) +
  scale_y_continuous(limits = c(-1, 1)) +
  geom_boxplot()
```


Show differences of means:

```{r fig.width=7, fig.height=5}
timings %>%
  filter(r == 0.1) %>%
  group_by(n, expr, prob) %>%
  summarize(time = mean(time)) %>%
  ungroup %>%
  group_by(n, prob) %>%
  mutate(id = seq_along(time)) %>%
  ungroup %>%
  spread(prob, time) %>%
  gather_("diff_to", "other_time", unique(timings$prob) %>% setdiff("uniform")) %>%
  mutate(rel_time_diff = (other_time - uniform) / uniform, uniform = NULL, other_time = NULL) %>%
  ggplot(aes(x=n, y=rel_time_diff, color=expr)) +
  facet_wrap(~diff_to) +
  scale_x_log10() +
  geom_line()
```



## Compare algorithms

Show differences of means:

```{r fig.width=7, fig.height=5}
timings %>%
  filter(r == 0.1) %>%
  filter(expr %in% c("crank", "ccrank")) %>%
  group_by(n, expr, prob) %>%
  summarize(time = mean(time)) %>%
  ungroup %>%
  group_by(n, expr) %>%
  mutate(id = seq_along(time)) %>%
  ungroup %>%
  (function(x) {
    spread(x, expr, time) %>%
    gather_("diff_to", "other_time", unique(x$expr) %>% setdiff("crank"))
  }) %>%
  mutate(rel_time_diff = (other_time - crank) / crank, crank = NULL, other_time = NULL) %>%
  ggplot(aes(x=n, y=rel_time_diff,  color=prob)) +
  facet_wrap(~diff_to) +
  scale_x_log10() +
  geom_line()
```



# Comparison of results
